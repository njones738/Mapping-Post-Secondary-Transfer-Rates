{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from geopy.distance import great_circle\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "df_pth = \"YOUR_PATH_HERE\"\n",
    "csc_dict_pth = \"YOUR_PATH_HERE\"\n",
    "\n",
    "output_pth = \"YOUR_PATH_HERE\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(df_pth,\n",
    "                 na_values = [None, np.nan, pd.NA, \"PrivacySuppressed\"], \n",
    "                 dtype = {\"UNITID\": \"str\",\n",
    "                          \"OPEID\": \"str\",\n",
    "                          \"OPEID6\": \"str\"}\n",
    "                ).drop(['NPCURL', 'ALIAS', 'T4APPROVALDATE', 'CIPTITLE1', 'CIPTITLE2', 'CIPTITLE3', 'CIPTITLE4', 'CIPTITLE5', 'CIPTITLE6', 'FEDSCHCD'], \n",
    "                       axis = 1)\n",
    "\n",
    "df = df[df[\"CONTROL\"] == 1]\n",
    "df = df[df[\"ICLEVEL\"] == 1]\n",
    "df = df[df[\"STABBR\"] != \"FM\"]\n",
    "df = df[df[\"STABBR\"] != \"GU\"]\n",
    "df = df[df[\"STABBR\"] != \"PR\"]\n",
    "df = df[df[\"STABBR\"] != \"VI\"]\n",
    "df = df[df[\"STABBR\"] != \"AK\"]\n",
    "df = df[df[\"STABBR\"] != \"HI\"]\n",
    "df = df[df[\"CURROPER\"] != 0]\n",
    "df = df.loc[df[\"DISTANCEONLY\"] != 1]\n",
    "df = df.loc[df[\"LATITUDE\"].isna() == False]\n",
    "# df = df[df[\"ADM_RATE\"].isna() == False]\n",
    "df = df[df[\"OMENRAP_ALL\"].isna() == False]\n",
    "\n",
    "\n",
    "missing_df = pd.DataFrame([[col, df.loc[df[col].isna()].shape[0]] for col in df.columns.to_list()], columns = [\"Variable\", \"nmiss\"])\n",
    "missing_df[\"n\"] = df.shape[0] - missing_df[\"nmiss\"]\n",
    "missing_df[\"n_pct\"] = missing_df[\"n\"] / df.shape[0]\n",
    "missing_df[\"nmiss_pct\"] = missing_df[\"nmiss\"] / df.shape[0]\n",
    "\n",
    "## Creates Data Dictionary\n",
    "# csc_dict_df = pd.merge(left = pd.read_csv(csc_dict_pth), right = missing_df, left_on = \"VARIABLE NAME\", right_on = \"Variable\", how = \"left\")\n",
    "# csc_dict_df = csc_dict_df[csc_dict_df[\"Variable\"].isin(df.columns.to_list())]\n",
    "# csc_dict_df = csc_dict_df.loc[:, [\"Variable\", \"NAME OF DATA ELEMENT\", \"n\", \"nmiss\", \"n_pct\", \"nmiss_pct\"]]\n",
    "# csc_dict_df[\"n\"] = csc_dict_df[\"n\"].astype('int64')\n",
    "# csc_dict_df[\"nmiss\"] = csc_dict_df[\"nmiss\"].astype('int64')\n",
    "# csc_dict_df[\"n_pct\"] = round(csc_dict_df[\"n_pct\"] * 100, 0).astype('int64')\n",
    "# csc_dict_df[\"nmiss_pct\"] = round(csc_dict_df[\"nmiss_pct\"] * 100, 0).astype('int64')\n",
    "# csc_dict_df.to_csv(csc_dict_pth)\n",
    "\n",
    "missing_df = missing_df[missing_df[\"nmiss\"] >= 0.9 * df.shape[0]]\n",
    "\n",
    "df.drop(missing_df.Variable.to_list() + [col for col in df.columns.to_list() if ((\"POOL\" in col) | (\"_SUPP\" in col))], axis = 1, inplace = True)\n",
    "\n",
    "del missing_df\n",
    "gc.collect()\n",
    "\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "df.sort_values(\"OMENRAP_ALL\", ascending = False, inplace = True)\n",
    "\n",
    "df[\"CATEG_TRANSFER_RATE\"] = -1\n",
    "\n",
    "df.loc[((df[\"OMENRAP_ALL\"] >= 0) & (df[\"OMENRAP_ALL\"] > 0.125)), \"CATEG_TRANSFER_RATE\"] = 0\n",
    "df.loc[((df[\"OMENRAP_ALL\"] >= 0.125) & (df[\"OMENRAP_ALL\"] > 0.25)), \"CATEG_TRANSFER_RATE\"] = 1\n",
    "df.loc[((df[\"OMENRAP_ALL\"] >= 0.375) & (df[\"OMENRAP_ALL\"] > 0.5)), \"CATEG_TRANSFER_RATE\"] = 2\n",
    "df.loc[((df[\"OMENRAP_ALL\"] >= 0.625) & (df[\"OMENRAP_ALL\"] > 0.75)), \"CATEG_TRANSFER_RATE\"] = 3\n",
    "df.loc[df[\"OMENRAP_ALL\"] >= 0.75, \"CATEG_TRANSFER_RATE\"] = 4\n",
    "\n",
    "df[\"CATEG2_TRANSFER_RATE\"] = 0\n",
    "df.loc[df[\"OMENRAP_ALL\"] >= 0.5, \"CATEG2_TRANSFER_RATE\"] = 1\n",
    "\n",
    "df[\"CATEG3_TRANSFER_RATE\"] = 0\n",
    "df.loc[df[\"OMENRAP_ALL\"] >= 0.375, \"CATEG3_TRANSFER_RATE\"] = 1\n",
    "\n",
    "df[\"CATEG_AVG_TRANSFER_RATE\"] = 0\n",
    "df.loc[df[\"OMENRAP_ALL\"] > df[\"OMENRAP_ALL\"].mean(), \"CATEG_AVG_TRANSFER_RATE\"] = 1\n",
    "\n",
    "df.to_csv(output_pth + \"csc.csv\", index = False)\n",
    "\n",
    "print(\"RESULTS:\", df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "\n",
    "print(f\"AVERAGE: {df['OMENRAP_ALL'].mean()}\")\n",
    "(\n",
    "    p9.ggplot(p9.aes(x = df[\"OMENRAP_ALL\"]))\n",
    "        + p9.geom_histogram(bins = 10, fill = \"lavender\", color = \"grey\")\n",
    "        + p9.theme_bw()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle\n",
    "from itertools import combinations\n",
    "\n",
    "def create_distance_matrix(df):\n",
    "    num_schools = len(df)\n",
    "    distance_matrix = np.zeros((num_schools, num_schools))\n",
    "    \n",
    "    for i in range(num_schools):\n",
    "        for j in range(i+1, num_schools):\n",
    "            distance = great_circle((df.loc[i, 'LATITUDE'], df.loc[i, 'LONGITUDE']),\n",
    "                                    (df.loc[j, 'LATITUDE'], df.loc[j, 'LONGITUDE'])).miles\n",
    "            distance_matrix[i, j] = distance\n",
    "            distance_matrix[j, i] = distance\n",
    "            \n",
    "    return distance_matrix\n",
    "\n",
    "def count_triangles_and_triples(matrix, g):\n",
    "    num_schools = len(matrix)\n",
    "    triangles = [0] * num_schools\n",
    "    triples = [0] * num_schools\n",
    "    \n",
    "    for i, j, k in combinations(range(num_schools), 3):\n",
    "        if matrix[i, j] <= g and matrix[j, k] <= g and matrix[k, i] <= g:\n",
    "            triangles[i] += 1\n",
    "            triangles[j] += 1\n",
    "            triangles[k] += 1\n",
    "        else:\n",
    "            if matrix[i, j] <= g and matrix[j, k] <= g:\n",
    "                triples[i] += 1\n",
    "                triples[j] += 1\n",
    "            if matrix[j, k] <= g and matrix[k, i] <= g:\n",
    "                triples[j] += 1\n",
    "                triples[k] += 1\n",
    "            if matrix[k, i] <= g and matrix[i, j] <= g:\n",
    "                triples[k] += 1\n",
    "                triples[i] += 1\n",
    "                \n",
    "    return triangles, triples\n",
    "\n",
    "def school_summary(df, g):\n",
    "    distance_matrix = create_distance_matrix(df)\n",
    "    triangles, triples = count_triangles_and_triples(distance_matrix, g)\n",
    "    \n",
    "    num_schools_within_g = []\n",
    "    avg_distance_within_g = []\n",
    "    \n",
    "    for row in distance_matrix:\n",
    "        schools_within_g = np.sum(row <= g) - 1\n",
    "        num_schools_within_g.append(schools_within_g)\n",
    "        \n",
    "        if schools_within_g > 0:\n",
    "            avg_distance = np.mean(row[row <= g][1:])\n",
    "            avg_distance_within_g.append(avg_distance)\n",
    "        else:\n",
    "            avg_distance_within_g.append(0)\n",
    "            \n",
    "    summary = pd.DataFrame({\n",
    "        'School': df.index,\n",
    "        f'Num_Schools_Within_{g}': num_schools_within_g,\n",
    "        f'Triangles_Within_{g}': triangles,\n",
    "        f'Triples_Within_{g}': triples,\n",
    "        f'Avg_Distance_Within_{g}': avg_distance_within_g\n",
    "    })\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = []\n",
    "for g in [50, 100]:\n",
    "    sum_list.append(school_summary(df, g = g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df] + sum_list, axis = 1)\n",
    "\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2[\"cluster_coeff_100\"].isna() == False].sort_values(\"cluster_coeff_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "\n",
    "df2[f\"cluster_coeff_{100}\"] = df2[f\"Triangles_Within_{100}\"] / df2[f\"Triples_Within_{100}\"]\n",
    "\n",
    "print(f\"AVERAGE: {df2['cluster_coeff_100'].mean()}\")\n",
    "# (\n",
    "#     p9.ggplot(p9.aes(x = df2[f\"Num_Schools_Within_{100}\"]))\n",
    "#         + p9.geom_histogram(bins = 10, fill = \"lavender\", color = \"grey\")\n",
    "#         + p9.theme_bw()\n",
    "# )\n",
    "(\n",
    "    p9.ggplot(p9.aes(x = df2[f\"cluster_coeff_{100}\"]))\n",
    "        + p9.geom_histogram(bins = 10, fill = \"lavender\", color = \"grey\")\n",
    "        + p9.theme_bw()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def school_analysis(df, g):\n",
    "    if not {'LATITUDE', 'LONGITUDE'}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'LATITUDE' and 'LONGITUDE' columns\")\n",
    "\n",
    "    n = len(df)\n",
    "    distances = np.zeros((n, n))\n",
    "\n",
    "    # Calculate distances between schools\n",
    "    for i, row_i in df.iterrows():\n",
    "        for j, row_j in df.iterrows():\n",
    "            if i != j:\n",
    "                distance = great_circle((row_i['LATITUDE'], row_i['LONGITUDE']), (row_j['LATITUDE'], row_j['LONGITUDE'])).miles\n",
    "                distances[i, j] = distance\n",
    "\n",
    "    # Calculate adjacency matrix for schools within g miles\n",
    "    adjacency_matrix = (distances <= g).astype(int)\n",
    "\n",
    "    # Count the number of schools within g miles\n",
    "    num_schools_within_g = np.sum(adjacency_matrix, axis=1) - 1\n",
    "\n",
    "    # Compute the cube of the adjacency matrix\n",
    "    adjacency_matrix_cube = np.linalg.matrix_power(adjacency_matrix, 3)\n",
    "\n",
    "    # Count the number of triangles at each node\n",
    "    triangles_per_node = np.diag(adjacency_matrix_cube) // 2\n",
    "\n",
    "    # Count the number of triples at each node\n",
    "    triples_per_node = np.sum(adjacency_matrix_cube, axis=1) - 3 * triangles_per_node\n",
    "\n",
    "    # Calculate the average distance for schools within g miles\n",
    "    avg_distance_within_g = np.sum(distances * adjacency_matrix, axis=1) / num_schools_within_g\n",
    "\n",
    "    clust_coeff = 0\n",
    "    if (isinstance(triples_per_node, int) & isinstance(triangles_per_node, int)):\n",
    "        if ((triples_per_node > 0) & (triangles_per_node > 0)):\n",
    "            clust_coeff = triangles_per_node / triples_per_node\n",
    "\n",
    "    # Create a new DataFrame with the results\n",
    "    results = pd.DataFrame({\n",
    "        f'num_schools_within_{g}_miles': num_schools_within_g,\n",
    "        f'num_triples_{g}_miles': triples_per_node,\n",
    "        f'num_triangles_{g}_miles': triangles_per_node,\n",
    "        f'avg_distance_within_{g}_miles': avg_distance_within_g,\n",
    "        f'clust_coeff': clust_coeff\n",
    "    })\n",
    "\n",
    "    return results\n",
    "\n",
    "df = pd.concat([df, school_analysis(df, 100)], axis = 1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, [\"INSTNM\", \"num_triangles_100_miles\", \"num_triples_100_miles\", \"clust_coeff\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from geopy.distance import great_circle\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_school_network(df, g, cat_col, student_counts):\n",
    "    if not {'LATITUDE', 'LONGITUDE', 'OMENRAP_ALL', cat_col}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'LATITUDE', 'LONGITUDE', 'OMENRAP_ALL', and the specified 'cat_col' columns\")\n",
    "\n",
    "    n = len(df)\n",
    "    distances = np.zeros((n, n))\n",
    "    weights = np.zeros((n, n))\n",
    "    \n",
    "\n",
    "    # Calculate distances between schools and edge weights\n",
    "    for i, row_i in df.iterrows():\n",
    "        for j, row_j in df.iterrows():\n",
    "            if i != j:\n",
    "                distance = great_circle((row_i['LATITUDE'], row_i['LONGITUDE']), (row_j['LATITUDE'], row_j['LONGITUDE'])).miles\n",
    "                distances[i, j] = distance\n",
    "                weights[i, j] = row_i['OMENRAP_ALL']\n",
    "\n",
    "    # Create the adjacency matrix for schools within g miles\n",
    "    adjacency_matrix = (distances <= g).astype(int)\n",
    "    np.fill_diagonal(adjacency_matrix, 0)\n",
    "\n",
    "    # Create the graph from the adjacency matrix\n",
    "    G = nx.from_numpy_matrix(adjacency_matrix)\n",
    "\n",
    "    # Add node attributes for latitude, longitude, and student count\n",
    "    nx.set_node_attributes(G, {i: {'pos': (row['LONGITUDE'], row['LATITUDE']), 'category': row[cat_col], 'students': student_counts[i]} for i, row in df.iterrows()})\n",
    "\n",
    "    # Add edge attributes for weights\n",
    "    nx.set_edge_attributes(G, {(i, j): {'weight': weights[i, j]} for i, j in G.edges})\n",
    "\n",
    "    # Create the map and plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 16), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.OCEAN, alpha=0.5)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale('10m'), alpha=0.5)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES.with_scale('10m'), linestyle=':', linewidth=0.33)\n",
    "    ax.set_extent([-130, -65, 25, 45])\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    custom_cmap = ListedColormap(['grey', 'gold'])\n",
    "\n",
    "    # Create a list of edges where one node is category 1 and the other is category 0\n",
    "    category_edges = [(u, v, d) for u, v, d in G.edges(data=True) if (G.nodes[u]['category'] == 1 and G.nodes[v]['category'] == 0) or (G.nodes[u]['category'] == 0 and G.nodes[v]['category'] == 1) or (G.nodes[u]['category'] == 1 and G.nodes[v]['category'] == 1)]\n",
    "\n",
    "    # Add the nodes and edges to the plot\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    edges = nx.draw_networkx_edges(G,\n",
    "                                pos,\n",
    "                                edgelist=category_edges,  # Draw only the category_edges\n",
    "                                width=[2 * d['weight'] for _, _, d in category_edges],\n",
    "                                alpha=[0.66 * d['weight'] for _, _, d in category_edges],\n",
    "                                edge_color='red',\n",
    "                                ax=ax)\n",
    "\n",
    "    nodes = nx.draw_networkx_nodes(G, \n",
    "                                    pos, \n",
    "                                    node_size= [0.001 * d['students'] for _, d in G.nodes(data=True)],  # Update node size based on student count\n",
    "                                    node_color=[d['category'] for _, d in G.nodes(data=True)], \n",
    "                                    cmap=custom_cmap,  # Use custom color map\n",
    "                                    alpha=[0.5 + 0.5 * d['category'] for _, d in G.nodes(data=True)], \n",
    "                                    ax=ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_school_network(df, g = 50, cat_col = \"CATEG3_TRANSFER_RATE\", student_counts = df[\"UGDS\"])\n",
    "plot_school_network(df, g = 100, cat_col = \"CATEG3_TRANSFER_RATE\", student_counts = df[\"UGDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_school_network(df, g = 50, cat_col = \"CATEG_AVG_TRANSFER_RATE\", student_counts = df[\"UGDS\"])\n",
    "plot_school_network(df, g = 100, cat_col = \"CATEG_AVG_TRANSFER_RATE\", student_counts = df[\"UGDS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_school_network(df, g = 110, cat_col = \"CATEG2_TRANSFER_RATE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from geopy.distance import great_circle\n",
    "\n",
    "def plot_school_network(df, g):\n",
    "    if not {'LATITUDE', 'LONGITUDE', 'OMENRAP_ALL'}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'LATITUDE', 'LONGITUDE', and 'OMENRAP_ALL' columns\")\n",
    "\n",
    "    n = len(df)\n",
    "    distances = np.zeros((n, n))\n",
    "    weights = np.zeros((n, n))\n",
    "\n",
    "    # Calculate distances between schools and edge weights\n",
    "    for i, row_i in df.iterrows():\n",
    "        for j, row_j in df.iterrows():\n",
    "            if i != j:\n",
    "                distance = great_circle((row_i['LATITUDE'], row_i['LONGITUDE']), (row_j['LATITUDE'], row_j['LONGITUDE'])).miles\n",
    "                distances[i, j] = distance\n",
    "                weights[i, j] = row_i['OMENRAP_ALL']\n",
    "\n",
    "    # Create the adjacency matrix for schools within g miles\n",
    "    adjacency_matrix = (distances <= g).astype(int)\n",
    "    np.fill_diagonal(adjacency_matrix, 0)\n",
    "\n",
    "    # Create the graph from the adjacency matrix\n",
    "    G = nx.from_numpy_matrix(adjacency_matrix)\n",
    "\n",
    "    # Add node attributes for latitude and longitude\n",
    "    nx.set_node_attributes(G, {i: {'pos': (row['LONGITUDE'], row['LATITUDE'])} for i, row in df.iterrows()})\n",
    "\n",
    "    # Add edge attributes for weights\n",
    "    nx.set_edge_attributes(G, {(i, j): {'weight': weights[i, j]} for i, j in G.edges})\n",
    "\n",
    "    # Create the map and plot\n",
    "    fig, ax = plt.subplots(figsize=(20, 16), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    \n",
    "    ax.coastlines()\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES.with_scale('10m'), linestyle=':', linewidth=0.33)\n",
    "    ax.set_extent([-130, -65, 25, 45])\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    # Add the nodes and edges to the plot\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    edges = nx.draw_networkx_edges(G,\n",
    "                                   pos,\n",
    "                                #    alpha = 1.0,\n",
    "                                   width = 1.0,\n",
    "                                   alpha = [d['weight'] for _, _, d in G.edges(data=True)],\n",
    "                                   edge_color = 'red',\n",
    "                                   ax = ax\n",
    "                                  )\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, node_size = 7.5, node_color='purple', alpha=0.8, ax=ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_school_network(df, g = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df.loc[:, ['INSTNM', 'LATITUDE', 'LONGITUDE', 'STABBR', 'OMENRAP_ALL', 'TRANS_4', 'DTRANS_4']].copy()\n",
    "\n",
    "sub_df[\"TRANS_4\"] = sub_df[\"TRANS_4\"].fillna(0)\n",
    "sub_df[\"DTRANS_4\"] = sub_df[\"DTRANS_4\"].fillna(0).astype('int64')\n",
    "\n",
    "sub_df[\"FTFT_TRANS_4yr_num\"] = (sub_df[\"TRANS_4\"] * sub_df[\"DTRANS_4\"]).fillna(0).astype('int64')\n",
    "\n",
    "sub_df.rename(columns = {\"OMENRAP_ALL\": \"WITHDRAW_RATE\", \"TRANS_4\": \"FTFT_TRANS_4yr_prop\", \"DTRANS_4\": \"FTFT_TRANS_4yr_denom\"})\n",
    "\n",
    "\n",
    "# Replace 'sub_df' with your actual DataFrame\n",
    "coords = sub_df[['LATITUDE', 'LONGITUDE']].to_numpy()\n",
    "distances = np.zeros((len(sub_df), len(sub_df)))\n",
    "\n",
    "for i in range(len(sub_df)):\n",
    "    lat1, lon1 = coords[i]\n",
    "    distances[i] = haversine(lat1, lon1, coords[:, 0], coords[:, 1])\n",
    "\n",
    "distance_df = pd.DataFrame(distances * 0.6213712, index = df[\"INSTNM\"], columns = df[\"INSTNM\"])\n",
    "\n",
    "# melted_distance_df = distance_df.reset_index().melt(id_vars='INSTNM', var_name='INSTNM2', value_name='distance_miles')\n",
    "# melted_distance_df = melted_distance_df.rename(columns={'INSTNM': 'INSTNM1'})\n",
    "\n",
    "# melted_distance_df = melted_distance_df[melted_distance_df[\"distance_miles\"] != 0]\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     distances_within_g = melted_distance_df.loc[(((melted_distance_df['INSTNM1'] == row['INSTNM']) | (melted_distance_df['INSTNM2'] == row['INSTNM'])) & (melted_distance_df['distance_miles'] <= 500)), ['INSTNM1', 'INSTNM2', 'distance_miles']]\n",
    "\n",
    "# distances_within_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_values(df, col, k, comparison='min'):\n",
    "    \"\"\"\n",
    "    Function that takes a DataFrame and returns a list where element i in the list\n",
    "    represents the number of rows with values less than k (or greater than k) in column i\n",
    "    of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame\n",
    "    k (float): The comparison value\n",
    "    comparison (str): 'min' for less than k, 'max' for greater than k. Defaults to 'min'.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the count of values less than or greater than k in each column.\n",
    "    \"\"\"\n",
    "\n",
    "    if comparison == 'min':\n",
    "        return \n",
    "    elif comparison == 'max':\n",
    "        return df[col][df[col] < k].count()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid comparison value. Accepted values are 'min' or 'max'.\")\n",
    "\n",
    "\n",
    "distance_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_universities_within_distance(distance_matrix, threshold):\n",
    "    return distance_matrix.applymap(lambda x: x <= threshold).sum(axis=1).fillna(0)\n",
    "\n",
    "count_universities_within_distance(distance_df, threshold = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"average_distance\"] = distance_df.mean().to_list()\n",
    "\n",
    "df[\"num_within_250m\"] = count_universities_within_distance(distance_df.reset_index(drop = True), threshold = 250)\n",
    "df[\"num_within_60m\"] = count_universities_within_distance(distance_df.reset_index(drop = True), threshold = 60)\n",
    "df[\"num_within_15m\"] = count_universities_within_distance(distance_df.reset_index(drop = True), threshold = 15)\n",
    "\n",
    "\n",
    "\n",
    "df.dtypes #head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def knn_regression(df, independent_vars, dependent_var, k):\n",
    "    # Standardize the independent variables\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df[independent_vars])\n",
    "    \n",
    "    # Fit the KNN model\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn.fit(X, df[dependent_var])\n",
    "    \n",
    "    # Predict the dependent variable using the KNN model\n",
    "    y_pred = knn.predict(X)\n",
    "    \n",
    "    # Create a scatterplot of the actual vs. predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df[dependent_var], y_pred, c = y_pred.astype('object'), alpha=0.5)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'KNN Regression with k={k}: Actual vs. Predicted {dependent_var}')\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# Example usage\n",
    "dependent_var = 'OMENRAP_ALL'\n",
    "independent_vars = ['local_clust_coeff_60', 'local_clust_coeff_110', 'local_clust_coeff_250', 'num_within_250m', 'num_within_60m'] # 'average_distance', 'num_within_250m', 'num_within_60m', , 'ADM_RATE_ALL'\n",
    "for col in independent_vars:\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "k = 5\n",
    "\n",
    "df[\"k_cluster\"] = knn_regression(df, independent_vars, dependent_var, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_distance_df = pd.merge(left = melted_distance_df,\n",
    "                              right = df.loc[:, [\"INSTNM\", \"OMENRAP_ALL\"]],\n",
    "                              left_on = \"INSTNM1\",\n",
    "                              right_on = \"INSTNM\",\n",
    "                              how = \"left\").drop(\"INSTNM\", axis = 1)\n",
    "\n",
    "melted_distance_df = pd.merge(left = melted_distance_df,\n",
    "                              right = df.loc[:, [\"INSTNM\", \"OMENRAP_ALL\"]],\n",
    "                              left_on = \"INSTNM2\",\n",
    "                              right_on = \"INSTNM\",\n",
    "                              how = \"left\").drop(\"INSTNM\", axis = 1)\n",
    "\n",
    "melted_distance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def stratified_scatterplot(df, x_col, y_col, cat_col):\n",
    "    if not {x_col, y_col, cat_col}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have '{}', '{}', and '{}' columns\".format(x_col, y_col, cat_col))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x=x_col, y=y_col, hue=cat_col, legend = True)\n",
    "    plt.title('Scatterplot of {} vs. {} Stratified by {}'.format(x_col, y_col, cat_col))\n",
    "    plt.show()\n",
    "\n",
    "stratified_scatterplot(df, 'k_cluster', 'average_distance', 'CONTROL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adjacency_matrix(df, g):\n",
    "    if not {'LATITUDE', 'LONGITUDE'}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'LATITUDE' and 'LONGITUDE' columns\")\n",
    "    \n",
    "    num_universities = len(df)\n",
    "    adj_matrix = np.zeros((num_universities, num_universities))\n",
    "\n",
    "    for idx1, row1 in df.iterrows():\n",
    "        for idx2, row2 in df.iterrows():\n",
    "            if idx1 != idx2:\n",
    "                distance = great_circle((row1['LATITUDE'], row1['LONGITUDE']), (row2['LATITUDE'], row2['LONGITUDE'])).miles\n",
    "                if distance <= g:\n",
    "                    adj_matrix[idx1, idx2] = 1\n",
    "\n",
    "    return adj_matrix\n",
    "\n",
    "def local_clustering_coefficients(adj_matrix):\n",
    "    adj_matrix = np.array(adj_matrix)\n",
    "    adj_matrix_sq = np.matmul(adj_matrix, adj_matrix) # Matrix multiplication to count the paths of length 2\n",
    "    adj_matrix_cube = np.matmul(adj_matrix_sq, adj_matrix) # Matrix multiplication to count the paths of length 3 (triangles)\n",
    "\n",
    "    degrees = adj_matrix.sum(axis=1) # Degrees of each node\n",
    "    num_triangles = np.diagonal(adj_matrix_cube) // 2 # Number of triangles for each node (divided by 2 to avoid double-counting)\n",
    "\n",
    "    clustering_coeffs = np.zeros(len(adj_matrix))\n",
    "\n",
    "    # Calculate the local clustering coefficient for each node\n",
    "    for i, (k_i, T_i) in enumerate(zip(degrees, num_triangles)):\n",
    "        if k_i > 1:\n",
    "            clustering_coeffs[i] = 2 * T_i / (k_i * (k_i - 1))\n",
    "        else:\n",
    "            clustering_coeffs[i] = 0\n",
    "\n",
    "    return clustering_coeffs\n",
    "\n",
    "\n",
    "adj_matrix_60 = adjacency_matrix(df, g = 60)\n",
    "adj_matrix_110 = adjacency_matrix(df, g = 110)\n",
    "adj_matrix_250 = adjacency_matrix(df, g = 250)\n",
    "\n",
    "df[\"local_clust_coeff_60\"] = local_clustering_coefficients(adj_matrix_60)\n",
    "df[\"local_clust_coeff_110\"] = local_clustering_coefficients(adj_matrix_110)\n",
    "df[\"local_clust_coeff_250\"] = local_clustering_coefficients(adj_matrix_250)\n",
    "\n",
    "df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_network(matrix, matrix_type='adjacency', threshold=None):\n",
    "    if matrix_type not in ['adjacency', 'distance']:\n",
    "        raise ValueError(\"matrix_type should be either 'adjacency' or 'distance'.\")\n",
    "\n",
    "    # Convert distance matrix to adjacency matrix\n",
    "    if matrix_type == 'distance':\n",
    "        if threshold is None:\n",
    "            raise ValueError(\"threshold value is required for distance matrix.\")\n",
    "        adjacency_matrix = (np.array(matrix) <= threshold).astype(int)\n",
    "    else:\n",
    "        adjacency_matrix = matrix\n",
    "\n",
    "    # Create a graph from the adjacency matrix\n",
    "    G = nx.from_numpy_matrix(adjacency_matrix)\n",
    "\n",
    "    # Plot the network graph\n",
    "    pos = nx.circular_layout(G)\n",
    "    nx.draw(G, pos, node_color='blue', with_labels = False)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_network(adj_matrix_60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"local_clust_coeff\"] = coeffs\n",
    "\n",
    "import plotnine as p9\n",
    "\n",
    "print([(\n",
    "    p9.ggplot(p9.aes(x = df[\"local_clust_coeff_60\"]))\n",
    "        + p9.geom_histogram(bins = 30, fill = \"lavender\", color = \"grey\")\n",
    "        + p9.theme_bw()\n",
    "),\n",
    "(\n",
    "    p9.ggplot(p9.aes(x = df[\"local_clust_coeff_110\"]))\n",
    "        + p9.geom_histogram(bins = 30, fill = \"lavender\", color = \"grey\")\n",
    "        + p9.theme_bw()\n",
    "),\n",
    "(\n",
    "    p9.ggplot(p9.aes(x = df[\"local_clust_coeff_250\"]))\n",
    "        + p9.geom_histogram(bins = 30, fill = \"lavender\", color = \"grey\")\n",
    "        + p9.theme_bw()\n",
    ")])\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, probplot\n",
    "\n",
    "def normality_test_and_visualizations(series):\n",
    "    # Perform the Shapiro-Wilk test for normality\n",
    "    stat, p = shapiro(series)\n",
    "    print(f'Shapiro-Wilk test statistic: {stat:.5f}')\n",
    "    print(f'p-value: {p:.5f}')\n",
    "    \n",
    "    if p > 0.05:\n",
    "        print(\"Fail to reject the null hypothesis - the data may be normally distributed.\")\n",
    "    else:\n",
    "        print(\"Reject the null hypothesis - the data may not be normally distributed.\")\n",
    "    \n",
    "    # Create a histogram for visual inspection\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(series, kde=True, color='blue', bins=30)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a Q-Q plot for visual inspection\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    probplot(series, dist='norm', plot=plt)\n",
    "    plt.xlabel('Theoretical Quantiles')\n",
    "    plt.ylabel('Ordered Values')\n",
    "    plt.title('Q-Q Plot')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "normality_test_and_visualizations(df[\"OMENRAP_ALL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"OMENRAP_ALL\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import probplot\n",
    "\n",
    "indep_vars = ['HCM2', 'MAIN', 'NUMBRANCH', 'PREDDEG', 'HIGHDEG', 'CONTROL', 'ST_FIPS', 'REGION', 'LOCALE', 'CCBASIC', 'CCUGPROF', 'CCSIZSET', 'ADM_RATE', 'ADM_RATE', 'ADM_RATE_ALL', 'SATVR25', 'SATVR75', 'SATMT25', 'SATMT75', 'SATVRMID', 'SATMTMID', 'ACTCM25', 'ACTCM75', 'ACTEN25', 'ACTEN75', 'ACTMT25', 'ACTMT75', 'ACTCMMID', 'ACTENMID', 'ACTMTMID', 'SAT_AVG', 'SAT_AVG_ALL', 'PCIP01', 'PCIP03', 'PCIP04', 'PCIP05', 'PCIP09', 'PCIP10', 'PCIP11', 'PCIP12', 'PCIP13', 'PCIP14', 'PCIP15', 'PCIP16', 'PCIP19', 'PCIP22', 'PCIP23', 'PCIP24', 'PCIP25', 'PCIP26', 'PCIP27', 'PCIP29', 'PCIP30', 'PCIP31', 'PCIP38', 'PCIP39', 'PCIP40', 'PCIP41', 'PCIP42', 'PCIP43', 'PCIP44', 'PCIP45', 'PCIP46', 'PCIP47', 'PCIP48', 'PCIP49', 'PCIP50', 'PCIP51', 'PCIP52', 'PCIP54', 'COSTT4_A', 'TUITIONFEE_IN', 'TUITIONFEE_OUT', 'TUITFTE', 'INEXPFTE', 'AVGFACSAL', 'PFTFAC', 'PCTPELL', 'OPENADMP', 'UGNONDS', 'GRADS']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "def create_colored_graph_v3(df, state_col='state'):\n",
    "    if not {'INSTNM', 'LATITUDE', 'LONGITUDE', state_col}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'INSTNM', 'LATITUDE', 'LONGITUDE', and '{}' columns\".format(state_col))\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Set the color palette\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Add nodes to the graph and assign colors\n",
    "    for idx, row in df.iterrows():\n",
    "        G.add_node(row['INSTNM'], lat=row['LATITUDE'], lon=row['LONGITUDE'], state=row[state_col])\n",
    "\n",
    "    # Get a list of unique states\n",
    "    unique_states = df[state_col].unique()\n",
    "\n",
    "    # Create a dictionary for state colors\n",
    "    state_colors = {state: sns.color_palette(\"colorblind\", len(unique_states))[i] for i, state in enumerate(unique_states)}\n",
    "\n",
    "    # Set node colors\n",
    "    node_colors = [state_colors[node[1]['state']] for node in G.nodes(data=True)]\n",
    "\n",
    "    # Create a Cartopy Albers Equal Area projection\n",
    "    projection = ccrs.AlbersEqualArea(central_longitude=-98.35, central_latitude=39.50)\n",
    "\n",
    "    # Draw the graph using the projected coordinates\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    ax = plt.axes(projection=projection)\n",
    "\n",
    "    # Draw the map features\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    # ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES.with_scale('110m'), linestyle=':', linewidth=0.33)\n",
    "    ax.set_extent([-155, -65, 10, 75])\n",
    "\n",
    "    # Draw the nodes\n",
    "    for node in G.nodes(data=True):\n",
    "        plt.plot(node[1]['lon'], node[1]['lat'], marker='o', color=state_colors[node[1]['state']], markersize=2.5, transform=ccrs.PlateCarree())\n",
    "\n",
    "    plt.title('Graph of Universities Colored by State')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv(\"your_college_data.csv\")\n",
    "# create_colored_graph_v3(df, state_col='state')\n",
    "\n",
    "create_colored_graph_v3(sub_df, state_col='STABBR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "def create_colored_graph_v2(df, state_col='state'):\n",
    "    if not {'INSTNM', 'LATITUDE', 'LONGITUDE', state_col}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'INSTNM', 'LATITUDE', 'LONGITUDE', and '{}' columns\".format(state_col))\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Set the color palette\n",
    "    sns.set_palette(\"colorblind\")\n",
    "\n",
    "    # Add nodes to the graph and assign colors\n",
    "    for idx, row in df.iterrows():\n",
    "        G.add_node(row['INSTNM'], lat=row['LATITUDE'], lon=row['LONGITUDE'], state=row[state_col])\n",
    "\n",
    "    # Get a list of unique states\n",
    "    unique_states = df[state_col].unique()\n",
    "\n",
    "    # Create a dictionary for state colors\n",
    "    state_colors = {state: sns.color_palette(\"coolwarm\", len(unique_states))[i] for i, state in enumerate(unique_states)}\n",
    "\n",
    "    # Set node colors\n",
    "    node_colors = [state_colors[node[1]['state']] for node in G.nodes(data=True)]\n",
    "\n",
    "    # Create a Cartopy Albers Equal Area projection\n",
    "    projection = ccrs.AlbersEqualArea(central_longitude=-98.35, central_latitude=39.50)\n",
    "\n",
    "    # Draw the graph using the projected coordinates\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.axes(projection=projection)\n",
    "\n",
    "    # Draw the map features\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.set_extent([-130, -70, 20, 60])\n",
    "\n",
    "    # Draw the nodes\n",
    "    for node in G.nodes(data=True):\n",
    "        plt.plot(node[1]['lon'], node[1]['lat'], marker='o', color=state_colors[node[1]['state']], markersize=5, transform=ccrs.PlateCarree())\n",
    "\n",
    "    plt.title('Graph of Universities Colored by State')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv(\"your_college_data.csv\")\n",
    "# create_colored_graph_v2(df, state_col='state')\n",
    "\n",
    "create_colored_graph_v2(sub_df, state_col='STABBR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_colored_graph(df, state_col='state'):\n",
    "    if not {'INSTNM', 'LATITUDE', 'LONGITUDE', state_col}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'INSTNM', 'LATITUDE', 'LONGITUDE', and '{}' columns\".format(state_col))\n",
    "\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Set the color palette\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Add nodes to the graph and assign colors\n",
    "    for idx, row in df.iterrows():\n",
    "        G.add_node(row['INSTNM'], pos=(row['LONGITUDE'], row['LATITUDE']), state=row[state_col])\n",
    "\n",
    "    # Get a list of unique states\n",
    "    unique_states = df[state_col].unique()\n",
    "\n",
    "    # Create a dictionary for state colors\n",
    "    state_colors = {state: sns.color_palette(\"coolwarm\", len(unique_states))[i] for i, state in enumerate(unique_states)}\n",
    "\n",
    "    # Set node colors\n",
    "    node_colors = [state_colors[node[1]['state']] for node in G.nodes(data=True)]\n",
    "\n",
    "    # Draw the graph\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw_networkx(G, pos=nx.get_node_attributes(G, 'pos'), node_size=50, node_color=node_colors, with_labels=False)\n",
    "    plt.title('Graph of Universities Colored by State')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "create_colored_graph(sub_df, state_col='STABBR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.distance import great_circle\n",
    "import networkx as nx\n",
    "import folium\n",
    "\n",
    "\n",
    "def INSTNM_distance_analysis_v2(df, g, k, k_kappa=\"max\"):\n",
    "    if not {'INSTNM', 'LATITUDE', 'LONGITUDE'}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'INSTNM', 'LATITUDE', and 'LONGITUDE' columns\")\n",
    "\n",
    "    # Calculate distances between universities\n",
    "    distances = []\n",
    "    for idx1, row1 in df.iterrows():\n",
    "        for idx2, row2 in df.iterrows():\n",
    "            if idx1 < idx2:\n",
    "                distance = great_circle((row1['LATITUDE'], row1['LONGITUDE']), (row2['LATITUDE'], row2['LONGITUDE'])).miles\n",
    "                distances.append([row1['INSTNM'], row2['INSTNM'], distance])\n",
    "\n",
    "    distance_df = pd.DataFrame(distances, columns=['INSTNM1', 'INSTNM2', 'distance_miles'])\n",
    "\n",
    "    # Create a histogram of distances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(distance_df['distance_miles'], bins='auto')\n",
    "    plt.xlabel('Distance (miles)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Distances between Universities')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the average distance and number of schools within 'g' miles for each INSTNM\n",
    "    INSTNM_stats = []\n",
    "    for idx, row in df.iterrows():\n",
    "        distances_within_g = distance_df.loc[(((distance_df['INSTNM1'] == row['INSTNM']) | (distance_df['INSTNM2'] == row['INSTNM'])) & (distance_df['distance_miles'] <= g)), 'distance_miles']\n",
    "        avg_distance = distances_within_g.mean()\n",
    "        num_schools = distances_within_g.count()\n",
    "        INSTNM_stats.append([row['INSTNM'], avg_distance, num_schools])\n",
    "\n",
    "    INSTNM_stats_df = pd.DataFrame(INSTNM_stats, columns=['INSTNM', 'average_distance', 'num_schools_within_g'])\n",
    "\n",
    "    # Create a scatterplot of average distance vs. number of schools within 'g' miles\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=INSTNM_stats_df, x='num_schools_within_g', y='average_distance')\n",
    "    plt.title(f'Average Distance vs. Number of Schools within {g} miles')\n",
    "    plt.show()\n",
    "\n",
    "    # Get the top 'k' universities based on the highest average distance\n",
    "    if k_kappa in [\"max\", 1, \"1\", \"largest\", \"large\"]:\n",
    "        top_k_universities = INSTNM_stats_df.nlargest(k, 'average_distance')['INSTNM']\n",
    "    if k_kappa in [\"min\", 0, \"0\", \"smallest\", \"small\"]:\n",
    "        top_k_universities = INSTNM_stats_df.nsmallest(k, 'average_distance')['INSTNM']\n",
    "    if k_kappa not in [\"max\", 1, \"1\", \"largest\", \"large\", \"min\", 0, \"0\", \"smallest\", \"small\"]:\n",
    "        print(\"k_kappa Error must be one of [max, 1, largest, large, min, 0, smallest, small]\")\n",
    "\n",
    "    # Create a heatmap of distances for the top 'k' universities\n",
    "    top_k_distance = distance_df.loc[(distance_df['INSTNM1'].isin(top_k_universities)) & (distance_df['INSTNM2'].isin(top_k_universities))]\n",
    "    heatmap_data = top_k_distance.pivot_table(index='INSTNM1', columns='INSTNM2', values='distance_miles')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='.1f')\n",
    "    plt.title(f'Heatmap of Distances between Top {k} Universities')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate positions for each INSTNM\n",
    "    edges = top_k_distance[['INSTNM1', 'INSTNM2']].values.tolist()\n",
    "    pos = {row['INSTNM']: (row['LONGITUDE'], row['LATITUDE']) for idx, row in df.iterrows()}\n",
    "\n",
    "    # Create a simple US map with vertices and edges\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "\n",
    "    ax.set_extent([-130, -65, 22, 50])\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, linestyle=':')\n",
    "\n",
    "    # Add vertices\n",
    "    for INSTNM, location in pos.items():\n",
    "        x, y = location[0], location[1]\n",
    "        ax.plot(x, y, marker='o', markersize=6, markeredgewidth=1, markeredgecolor='k', markerfacecolor='blue', alpha=0.8, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add edges\n",
    "    for edge in edges:\n",
    "        INSTNM1 = edge[0]\n",
    "        INSTNM2 = edge[1]\n",
    "        lon1, lat1 = pos[INSTNM1]\n",
    "        lon2, lat2 = pos[INSTNM2]\n",
    "        ax.plot([lon1, lon2], [lat1, lat2], linewidth=2, color='blue', alpha=0.6, transform=ccrs.PlateCarree())\n",
    "\n",
    "    plt.title(\"Vertices and Edges for Top {} Universities\".format(k))\n",
    "    plt.show()\n",
    "\n",
    "    return distance_df, INSTNM_stats_df, top_k_universities\n",
    "\n",
    "distance_df, INSTNM_stats_df, top_k_universities = INSTNM_distance_analysis_v2(sub_df, g = 500, k = 25, k_kappa = \"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.distance import great_circle\n",
    "import networkx as nx\n",
    "import folium\n",
    "\n",
    "def INSTNM_distance_analysis(df, g, k, k_kappa = \"max\"):\n",
    "    if not {'INSTNM', 'LATITUDE', 'LONGITUDE'}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'INSTNM', 'LATITUDE', and 'LONGITUDE' columns\")\n",
    "\n",
    "    # Calculate distances between universities\n",
    "    distances = []\n",
    "    for idx1, row1 in df.iterrows():\n",
    "        for idx2, row2 in df.iterrows():\n",
    "            if idx1 < idx2:\n",
    "                distance = great_circle((row1['LATITUDE'], row1['LONGITUDE']), (row2['LATITUDE'], row2['LONGITUDE'])).miles\n",
    "                distances.append([row1['INSTNM'], row2['INSTNM'], distance])\n",
    "\n",
    "    distance_df = pd.DataFrame(distances, columns=['INSTNM1', 'INSTNM2', 'distance_miles'])\n",
    "\n",
    "    # Create a histogram of distances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(distance_df['distance_miles'], bins='auto')\n",
    "    plt.xlabel('Distance (miles)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Distances between Universities')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the average distance and number of schools within 'g' miles for each INSTNM\n",
    "    INSTNM_stats = []\n",
    "    for idx, row in df.iterrows():\n",
    "        distances_within_g = distance_df.loc[(((distance_df['INSTNM1'] == row['INSTNM']) | (distance_df['INSTNM2'] == row['INSTNM'])) & (distance_df['distance_miles'] <= g)), 'distance_miles']\n",
    "        avg_distance = distances_within_g.mean()\n",
    "        num_schools = distances_within_g.count()\n",
    "        INSTNM_stats.append([row['INSTNM'], avg_distance, num_schools])\n",
    "\n",
    "    INSTNM_stats_df = pd.DataFrame(INSTNM_stats, columns=['INSTNM', 'average_distance', 'num_schools_within_g'])\n",
    "\n",
    "    # Create a scatterplot of average distance vs. number of schools within 'g' miles\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=INSTNM_stats_df, x='num_schools_within_g', y='average_distance')\n",
    "    plt.title(f'Average Distance vs. Number of Schools within {g} miles')\n",
    "    plt.show()\n",
    "\n",
    "    # Get the top 'k' universities based on the highest average distance\n",
    "    if k_kappa in [\"max\", 1, \"1\", \"largest\", \"large\"]:\n",
    "        top_k_universities = INSTNM_stats_df.nlargest(k, 'average_distance')['INSTNM']\n",
    "    if k_kappa in [\"min\", 0, \"0\", \"smallest\", \"small\"]:\n",
    "        top_k_universities = INSTNM_stats_df.nsmallest(k, 'average_distance')['INSTNM']\n",
    "    if k_kappa not in [\"max\", 1, \"1\", \"largest\", \"large\", \"min\", 0, \"0\", \"smallest\", \"small\"]:\n",
    "        print(\"k_kappa Error must be one of [max, 1, largest, large, min, 0, smallest, small]\")\n",
    "\n",
    "    # Create a heatmap of distances for the top 'k' universities\n",
    "    top_k_distance = distance_df.loc[(distance_df['INSTNM1'].isin(top_k_universities)) & (distance_df['INSTNM2'].isin(top_k_universities))]\n",
    "    heatmap_data = top_k_distance.pivot_table(index='INSTNM1', columns='INSTNM2', values='distance_miles')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt='.1f')\n",
    "    plt.title(f'Heatmap of Distances between Top {k} Universities')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a cartographic map with vertices where each INSTNM is and edges to the top 'k' universities\n",
    "    edges = top_k_distance[['INSTNM1', 'INSTNM2']].values.tolist()\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    pos = {row['INSTNM']: (row['LONGITUDE'], row['LATITUDE']) for idx, row in df.iterrows()}\n",
    "    nx.set_node_attributes(G, pos, 'pos')\n",
    "\n",
    "    # Calculate the center of the map\n",
    "    center_LATITUDE = df['LATITUDE'].mean()\n",
    "    center_LONGITUDE = df['LONGITUDE'].mean()\n",
    "\n",
    "    map = folium.Map(location=[center_LATITUDE, center_LONGITUDE], zoom_start=5)\n",
    "\n",
    "    # Add vertices and edges to the map\n",
    "    for edge in edges:\n",
    "        INSTNM1 = edge[0]\n",
    "        INSTNM2 = edge[1]\n",
    "        coordinates = [pos[INSTNM1], pos[INSTNM2]]\n",
    "        folium.PolyLine(coordinates, color='blue', weight=5, opacity=1).add_to(map)\n",
    "\n",
    "    for INSTNM, location in pos.items():\n",
    "        folium.Circle(location, radius=1000, popup=INSTNM, color='blue', fill=True, fill_color='blue', fill_opacity=1).add_to(map)\n",
    "\n",
    "\n",
    "    map.save(\"INSTNM_map.html\")\n",
    "    print(\"Cartographic map with vertices and edges saved as 'INSTNM_map.html'\")\n",
    "\n",
    "    return distance_df, INSTNM_stats_df, top_k_universities\n",
    "\n",
    "\n",
    "INSTNM_distance_analysis(sub_df, g = 500, k = 10, k_kappa = \"min\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth's radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Replace 'sub_df' with your actual DataFrame\n",
    "coords = sub_df[['LATITUDE', 'LONGITUDE']].to_numpy()\n",
    "distances = np.zeros((len(sub_df), len(sub_df)))\n",
    "\n",
    "for i in range(len(sub_df)):\n",
    "    lat1, lon1 = coords[i]\n",
    "    distances[i] = haversine(lat1, lon1, coords[:, 0], coords[:, 1])\n",
    "\n",
    "distance_df = pd.DataFrame(distances * 0.6213712, index = df[\"INSTNM\"], columns = df[\"INSTNM\"])\n",
    "\n",
    "melted_distance_df = distance_df.reset_index().melt(id_vars='INSTNM', var_name='INSTNM2', value_name='distance_miles')\n",
    "melted_distance_df = melted_distance_df.rename(columns={'INSTNM': 'INSTNM1'})\n",
    "\n",
    "melted_distance_df = melted_distance_df[melted_distance_df[\"distance_miles\"] != 0]\n",
    "\n",
    "print(f\"{melted_distance_df.shape}\")\n",
    "melted_distance_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def INSTNM_distance_analysis(df, n = 25, ):\n",
    "    # Ensure input DataFrame has the required columns\n",
    "    if not {'INSTNM1', 'INSTNM2', 'distance_miles'}.issubset(df.columns):\n",
    "        raise ValueError(\"DataFrame must have 'INSTNM1', 'INSTNM2', and 'distance_miles' columns\")\n",
    "\n",
    "    # Create a histogram of distances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df['distance_miles'], bins='auto')\n",
    "    plt.xlabel('Distance (miles)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Distances between Universities')\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the average distance for each INSTNM\n",
    "    avg_distance = (df.groupby('INSTNM1')['distance_miles'].mean() + df.groupby('INSTNM2')['distance_miles'].mean()) / 2\n",
    "\n",
    "    # Get the top 25 universities with the highest average distance\n",
    "    top_25_universities = avg_distance.nlargest(n).index\n",
    "\n",
    "    # Create a heatmap of distances for the top 25 universities\n",
    "    top_25_distance = df.loc[(df['INSTNM1'].isin(top_25_universities)) & (df['INSTNM2'].isin(top_25_universities))]\n",
    "\n",
    "    # Pivot the DataFrame to create a matrix suitable for heatmap\n",
    "    heatmap_data = top_25_distance.pivot_table(index='INSTNM1', columns='INSTNM2', values='distance_miles', aggfunc=np.mean)\n",
    "\n",
    "    # Generate the heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt='.1f', linewidths=.5)\n",
    "    plt.title('Heatmap of Distances between Top 25 Universities')\n",
    "    plt.show()\n",
    "\n",
    "INSTNM_distance_analysis(melted_distance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_distance_df.distance_miles.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_distance_df.distance_miles.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_distance_df.distance_miles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
